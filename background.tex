\chapter{Background}

\section{Machine Learning}
Machine learning techniques are used to extract information from data, or make some predictions about the data. This chapter briefly explains methods and techniques used in, or required to understand the proceeding chapters.

\subsection{Feature Selection}
Feature selection is the task of selecting important features to the problem at hand. It becomes particularly a hard task when the number of features in the data is of a higher magnitude compared to the number of given samples. Table~\ref{tab:sample-sample-size} shows an example number of samples vs. number of features in a typical data. One of the challenges when dealing with such a large number of features is that if enough number of features have a probability distribution independent of the outcome, some of them might falsely seem correlated with the outcome. Another obstacle comes from the fact that our features are not independent and they function in complex networks. As a result, features should be considered in groups, which is a combinatorial and intractable problem.

\begin{table}[th!]
  \centering
  \begin{tabular}{p{3cm} p{3.5cm} p{4cm}}
    \hline
    \multicolumn{3}{c}{Sample Data} \\
    \cline{1-3}
    Sample Count   & Gene Expression Data Feature Count & 450K Methylation Chip Data Feature Count \\
    \hline
    $500$      & $\approx 20,000$    & $\approx 450,000$   \\
    \hline
  \end{tabular}
  \caption{An example number of samples and features in our usual data}
  \label{tab:sample-sample-size}
\end{table}

We have used correlation~\cite{correlation}, mutual information~\cite{mutual-information}, and $l_1$-regularized methods~\cite{l1-regularized} as techniques to select features.

\subsection{Classification}
Classification is the problem of putting data into different classes~\cite{statistical-learning}. During the training phase, the matrix $X_{samples \times features}$ is given as the input and $y_{samples}$ as the desired output. The vector $y$ has values from a discrete set. If the set has only two distinct values, the problem is called a binary classification.

Logistic regression~\cite{logistic-regression1,logistic-regression2}, Support Vector Machines (SVM)~\cite{svm1,svm2}, and decision trees~\cite[TODO:chapter]{statistical-learning} are examples of classification methods.

\subsection{Regression}
In statistics, predicting a continues output value given an input data is called regression~\cite[TODO:chapter]{statistical-learning}. Regression and classification differ in their desired output type. In regression the output is continues in contrast to classification in which the output is a discrete value.

Linear regression~\cite[TODO:chapter]{statistical-learning}, Gaussian processes~\cite{gaussian-processes}, and kernel based regression~\cite[TODO:chapter]{learning-with-kernels} are some available methods here.

\subsection{Regularization}
Building a machine to predict the outcome with a good performance on the training set is easy if the number of features in the data is large enough compared to the number of samples, even if features are drawn from a random background probability distribution independent of the outcome. But the trained machine will perform poorly on the unseen test samples. This phenomenon is called overfitting. One way to prevent overfitting is to select potential features before training a selected model.

Another way to tackle the problem is to reduce the complexity of the models. This can be done via regularization. The $l_1$-regularization is an appropriate tool when the intention is to reduce the number of features a model takes into account for prediction as well as its complexity~\cite{l1-regularized}.

Assume a model minimizes a loss function $E(X, Y)$, where $X$ is the input matrix and $Y$ is the output vector or matrix. In most regression models the loss function is defined as:

\begin{align}
  E(X, Y) = \parallel Y - X \beta \parallel_2
  \label{frm:loss-function}
\end{align}

The optimization algorithm finds a $\beta$ that minimizes the loss function in Formula~\ref{frm:loss-function}. Having enough number of features, the optimization algorithm might find a $\beta$ that gives a perfect loss, i.e. 0. But in noisy environments the resulting $\beta$ is probably not the real $\beta$ of the underlying model producing the data. The vector $\beta$ might also have some extreme values that are likely not desired. Penalizing the \emph{size} of $\beta$ as shown in Formula~\ref{frm:regularized} will address the abovementioned concern. The size of a vector in this context is represented by its $l_1$ or $l_2$ norm as defined in Formula~\ref{frm:lp-norm}.


\begin{align}
  \parallel \beta \parallel_p := \left(\sum\limits_{i=1}^n \mid \beta_i \mid^p \right)^{1/p}
  \label{frm:lp-norm}
\end{align}
\begin{align}
  &E(X, Y) + \alpha \parallel \beta \parallel_2 = \parallel Y - X \beta \parallel_2 + \alpha \parallel \beta \parallel_2 \nonumber \\
  &\text{or} \nonumber \\
  &E(X, Y) + \alpha \parallel \beta \parallel_1 = \parallel Y - X \beta \parallel_2 + \alpha \parallel \beta \parallel_1
  \label{frm:regularized}
\end{align}

\subsection{Empirical Risk Minimization}
We need to minimize the risk of the error of the predictor~\cite[p.~20]{thenatureofstatisticallearningtheory}.

\subsection{Support Vector Machines}
Support vector machines (SVM) can be used both for regression and classification tasks~\cite{svm2, svr1}. As a classifier, SVM finds an optimal hyperplane to separate data points in the feature space by maximizing the hyperplane's margin to the nearest point. Therefore given a test data point, its side with regard to the hyperplane determines its class. As a regressor however, SVM finds an optimal hyperplane to interpolate given data points by minimizing the hyperplane's distance from data points.

Formally speaking, given data-set $\mathcal{D}$ of $n$ data points:

\begin{align}
  \mathcal{D}={(\mathbf{x}_i, y_i) | \mathbf{x}_i \in \mathbb{R}^p, y_i \in {-1, 1}}_{i=1}^{n}
\end{align}

where $\mathbf{x}_i$ is a real vector of length $p$, and $y_i$ is either $1$ or $-1$. A $p$-dimensional hyperplane, characterized by its normal vector $\mathbf{w}$ and its intercept $\mathbf{b}$, is the set of points $\mathbf{x}$ that fit in Formula~\ref{frm:hyperplane}.

\begin{align}
  \mathbf{w} . \mathbf{x} - b = 0
  \label{frm:hyperplane}
\end{align}

Now consider two hyperplanes on both sides of the abovementioned hyperplane as formulated bellow:

\begin{align}
  \mathbf{w} . \mathbf{x} - b &= 1 \nonumber \\
  \mathbf{w} . \mathbf{x} - b &= -1
\end{align}

The distance between each of these hyperplanes and the on in the middle is $\frac{1}{\parallel \mathbf{w} \parallel}$. Therefore the distance between the two of them is $\frac{2}{\parallel \mathbf{w} \parallel}$. For now we assume the data is linearly separable in its feature space, i.e. there exists a hyperplane that separates the data into two classes without error. Such a hyperplane satisfies the following constraint:

\begin{align}
  y_i (\mathbf{w} . \mathbf{x}_i - b)\geq 1 \text{ for all } 1 \leq i \leq n
\end{align}

An optimal hyperplane is one such that it maximizes the margin; hence formulated as Formula~\ref{frm:svm2}. An illustration of the optimal solution is presented in Fig.~\ref{fig:svm-hyperplane}.

\begin{align}
  &\arg\max_{(\mathbf{w},b)}\frac{1}{\|\mathbf{w}\|} \nonumber \\
  &\text{s.t.} \nonumber \\
  &y_i (\mathbf{w} . \mathbf{x}_i - b)\geq 1 \text{ for all } 1 \leq i \leq n
  \label{frm:svm2}
\end{align}

\begin{figure}[!ht]
  \centering
  \includegraphics[width=.5\textwidth]{figs/background/Svm_max_sep_hyperplane_with_margin}
  \caption{Illustration of the optimal hyperplane in a support vector machine model, for a 2-dimensional data.}
  \label{fig:svm-hyperplane}
\end{figure}

However, for an easier optimization and mathematical convenience, the above optimization problem is usually formulated as Formula~\ref{frm:svm1} which has the same solution as $\mathbf{w}$ and $b$.

\begin{align}
  &\arg\min_{(\mathbf{w},b)}\frac{1}{2}\|\mathbf{w}\|^2 \nonumber \\
  &\text{s.t.} \nonumber \\
  &y_i (\mathbf{w} . \mathbf{x}_i - b)\geq 1 \text{ for all } 1 \leq i \leq n
  \label{frm:svm1}
\end{align}

which can be written as Formula~\ref{frm:svm3} after introducing Karush-Kuhn-Tucker (KKT) multipliers~\cite{kkt-orig}.

\begin{align}
  &\arg\min_{\mathbf{w},b } \max_{\boldsymbol{\alpha}} \left\{ \frac{1}{2}\|\mathbf{w}\|^2 - \sum_{i=1}^{n}{\alpha_i[y_i(\mathbf{w}\cdot \mathbf{x_i} - b)-1]} \right\} \nonumber \\
  &\text{s.t.} \nonumber \\
  &\alpha_i \geq 0 \text{ for } 1 \leq i \leq n \nonumber \\
  \label{frm:svm3}
\end{align}

Multipliers $\alpha_i$ will be $0$ for each $\mathbf{x}_i$ that does not lie on either of the marginal hyperplanes. For example in Figure~\ref{fig:svm-hyperplane}, $\alpha_i$ is non-zero for only three of the data points; the ones that are exactly on either of the marginal lines. The corresponding $\mathbf{x}_i$ for which $\alpha_i$ is non-zero, are \emph{support vectors}.

It can be shown that Formula~\ref{frm:svm-dual1} is a dual of the optimization problem defined in Formula~\ref{frm:svm3}~\cite[TODO:chapter]{learning-with-kernels}.

\begin{align}
  &\arg\max_{\boldsymbol{\alpha}}\left\{\sum_{i=1}^n \alpha_i - \frac{1}{2}\sum_{i, j} \alpha_i \alpha_j y_i y_j \mathbf{x}_i^T \mathbf{x}_j\right\} \nonumber \\
  &\text{s.t.} \nonumber \\
  &\alpha_i \geq 0 \text{ for } 1 \leq i \leq n \nonumber \\
  &\sum_{i=1}^{n}\alpha_i y_i = 0
  \label{frm:svm-dual1}
\end{align}

Now assume the following notations and definitions:

\begin{align}
  \phi(\mathbf{x}) &:= \mathbf{x} \nonumber \\
  \langle \mathbf{x}_i, \mathbf{x}_j \rangle &:= \mathbf{x}_i^T \mathbf{x}_j \nonumber \\
  k(\mathbf{x}_i, \mathbf{x}_j) &:= \langle \phi(\mathbf{x}_i), \phi(\mathbf{x}_j) \rangle
  \label{frm:svm-ktrick1}
\end{align}

Putting function $k$ in Formula~\ref{frm:svm-dual1}, the SVM's optimization problem can be written as:

\begin{align}
  &\arg\max_{\boldsymbol{\alpha}}\left\{\sum_{i=1}^n \alpha_i - \frac{1}{2}\sum_{i, j} \alpha_i \alpha_j y_i y_j k(\mathbf{x}_i, \mathbf{x}_j)\right\} \nonumber \\
  &\text{s.t.} \nonumber \\
  &\alpha_i \geq 0 \text{ for } 1 \leq i \leq n \nonumber \\
  &\sum_{i=1}^{n}\alpha_i y_i = 0
  \label{frm:svm-dual2}
\end{align}

The identity function used in Formula~\ref{frm:svm-ktrick1} is not the only option. We can transform the data into another feature space using a different $\phi(.)$, and then use dot-product in that space. This is useful for cases that the data is not linearly separable in its original feature space, but linearly separable using a non-linear transformation.

Using Mercer's theorem~\cite{mercer-theorem} and its corollary Mercer's condition, it can be shown that any function $k$ satisfying the following condition can be used as a \emph{kernel} in Formula~\ref{frm:svm-dual2}~\cite[TODO:chapter]{learning-with-kernels}.

\begin{align}
  \forall \mathcal{D}, \forall c_i, c_j \in \mathbb{R}: \sum_{i=1}^n\sum_{j=1}^n k(\mathbf{x}_i, \mathbf{x}_j) c_i c_j \geq 0
  \label{frm:positive-definite}
\end{align}

Arguably, other than dot-product, the most famous kernel function $k$ satisfying the above condition is the \emph{Gaussian kernel}, also known as the \emph{radial basis function (RBF) kernel}:

\begin{align}
  &k(\mathbf{x_i}, \mathbf{x_j}) = \exp\left(-\frac{||\mathbf{x_i} - \mathbf{x_j}||^2}{2\sigma^2}\right) \nonumber \\
  &\sigma \in \mathbb{R}
  \label{frm:rbf-kernel1}
\end{align}

Many implementations use a different formulation which uses a different parametrization, using $\gamma=\frac{1}{2\sigma^2}$:

\begin{align}
  &k(\mathbf{x_i}, \mathbf{x_j}) = \exp\left(-\gamma||\mathbf{x_i} - \mathbf{x_j}||^2\right) \nonumber \\
  &\gamma \in \mathbb{R}^+
  \label{frm:rbf-kernel2}
\end{align}

\subsection{Gaussian Processes}

\subsection{Boosting and Ensemble Methods}
For a given prediction problem the idea of boosting is to find an optimal combination of classifiers, also called ``weak learners''~\cite{ensemble2002}. There are many methods of finding the optimal combination of such weak learners, two of which are stochastic gradient boosting~\cite{friedman2002stochastic} and AdaBoost~\cite{adaboost97}. Stochastic gradient boosting tries to estimate the gradients of the loss function and train each individual weak learner in a way that best improves the loss function. AdaBoost tries to identify samples among given data samples that are harder to classify, and gives them more weight in the process of training individual weak learners. One way of improving AdaBoost is to take into account the confidences of predictions given by weak learners if possible and use estimated confidences in the voting process~\cite{adaboost99improved}.

\section{Graphs}
A graph $G$ is a set of vertices (nodes) $V$, and a set of edges $E=\left\{(v_i, v_j) | v_i, v_j \in V\right\}$ that connect vertices in $V$. A graph can be directed or undirected. In directed graphs, edges have direction, i.e. edge $(s, t)$ is different than the edge $(t, s)$. In other words, the following list shows the possible sets of edges regarding vertices $s$ and $t$ in a directed graph:

\begin{align}
  E &= \{\} \nonumber \\
  E &= \{(s, t)\} \nonumber \\
  E &= \{(t, s)\} \nonumber \\
  E &= \{(s, t), (t, s)\}
\end{align}

In an undirected graph however, edges $(s, t)$ and $(t, s)$ are identical, and the can in fact be represented as a set ${s, t}$ instead of an ordered pair.

Graphs can also be weighted or not. If a graph $G$ is weighted, then there is a weight assigned to each edge of the graph. We use $w_{s,t}$ to note the weight of the edge $(s,t)$. In undirected graphs, $w_{s, t}$ is always the same as $w_{t, s}$. Sometimes the weight of an edge is referred to as the length of an edge and noted as $l_{s,t}$ depending on the context in the literature.
A graph $G$ is connected if there is at least one path between every given two vertices on the graph.

\subsection{The Shortest Path Problem}
\emph{The shortest path problem} is to find a path between two vertices $s$ and $t$ such that the total weight of the path is the minimum. In unweighted graphs, the weight of each edge is considered to be $1$. Figure~\ref{fig:shortest-path1} highlights the shortest path between vertices $A$ and $F$ on the given weighted directed graph.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.5\textwidth]{figs/background/Shortest_path_with_direct_weights}
  \caption{A given weighted directed graph and the highlighted shortest path between vertices $A$ and $F$.}
  \label{fig:shortest-path1}
\end{figure}

Some prominent algorithms to solve the shortest path problem are Dijkstra's~\cite{dijkstra}, Bellman-Ford~\cite{bellman-ford}, and Floyd-Warshall~\cite{floyd-warshall} algorithms. Dijkstra's algorithm applies to the single-source shortest path problem on graphs with non-negative weight values with time complexity $O(|E| + |V| \log |V|)$~\cite{fredman1987fibonacci}, whereas Bellman-Ford algorithm works on graphs with also negative weights having time complexity $O(|V| |E|)$. Floyd-Warshall algorithm, on the other hand, applies to all pairs shortest path problem, i.e. it finds shortest paths between all pairs of vertices, on graphs with negative and non-negative weight values and its time complexity is $O(|V|^3)$.

\subsection{The $k$ Shortest Path Problem}
\emph{The $k$ shortest path problem} is to find not only one, but $k$ paths from $s$ to $t$ such that their corresponding total weight is the least $k$ among all distinct possible paths from $s$ to $t$. Two settings of the problem are due to whether or not allowing loops in the paths. For the case when the goal is to find $k$ best shortest paths from a single source to all other nodes, Jin Y. Yen published an algorithm of the time complexity $O(k |V|(|E|+|V|\log|V|))$ in 1971 for the loopless setting which still has the best available time complexity available~\cite{yen1971finding}. It is possible to achieve better worst case time complexity if we let loops in the paths. In 1998 Eppstein came up with an algorithm with $O(|E| + |V|\log|V| + |V| k)$ time complexity, and $O(|E| + |V|\log|V| + k)$ if the problem is reduced to the single source single destination case~\cite{eppstein1998finding}. There has been improvements to the Eppstein's algorithm, but the worst case time complexity has not been improved.

In our case, the graph is a directed acyclic graph (DAG), i.e. there are no loops in the graph. Therefore despite we require paths to be loopless, Eppstein's algorithm is sufficient and gives desirable paths. Here we give an intuitive overview of the algorithm and postpone our use case in detail to Chapter~\ref{sec:fcs}.

First we need to introduce some concepts and notations, and for the sake of easier reference to the original work, we keep the notation as the work done by Eppstein.
Assume the problem is to find the $k$ shortest paths from $s$ to $t$ on a connected directed graph $G$. Then consider the following:
\begin{itemize}
\item $T$: a single destination shortest path tree and $t$ be its destination, i.e. $T$ includes all vertices of $G$ and a shortest path from each node to $t$.
\item $d(v_i, v_j)$: the weight of a shortest path from $v_i$ to $v_j$, or in other words the distance between the two vertices.
\item $head(e), tail(e)$: if $e$ is $(v_i, v_j)$, $head$ and $tail$ of $e$ are $v_i$ and $v_j$ respectively.
\item $l(e)$: weight or length of edge $e$.
\item $\delta(e)$: intuitively the cost of including $e$ in a shortest path to $t$, defined as:
  \begin{align}
    \delta(e) = l(e) + d(head(e), t) - d(tail(e), t)
  \end{align}
\end{itemize}

If the edge $e$ is not a part of $T$, it is a \emph{sidetrack} and the cost of including it in a path to $t$ is non-negative~\cite[Lemma 1]{eppstein1998finding}.

A key point to understanding the algorithm is the way paths are represented. A path $p$ from $s$ to $t$ can be represented by the list of \emph{sidetrack} edges it includes. If the path $p$ includes only one \emph{sidetrack} edge $(v_i, v_j)$, it means the path is the shortest path from $s$ to $v_i$, then the edge $(v_i, v_j)$, and then the shortest path from $v_j$ to $t$. The set $sidetracks(p)$ includes all edges in $p$ that are not in the shortest path tree $T$, i.e. they are in $G - T$.

To calculate the length of the path $p$ we have~\cite[Lemma 2]{eppstein1998finding}:
\begin{align}
  l(p) = d(s, t) + \sum_{e \in sidetracks(p)} \delta(e)
\end{align}

Let $S = sidetracks(p)$ to be the sequence of edges of $p$ that are in $G - T$. Then we define $path(S)$ to be the path $p$. We also define $prefix(S)$ to be the sequence of edges in $S$ except the last one. Therefore $prefix(S)$ can define a path as $prefpath(p) = path(prefix(S))$.

Next we have: if the path $p$ is from $s$ to $t$ in $G$ and has a nonempty $sidetracks(p)$, then $l(p) \geq l(prefpath(p)$~\cite[Lemma 3]{eppstein1998finding}. As a corollary of Lemma 2 and 3 we can construct a natural tree of paths which is also a heap style tree. It is a tree in a way that each node is a path $p$, and it has all possible paths $p'$ for which $prefpath(p') = path(p)$. It is also a heap style tree in a way that the length of a parent node is less than or equal to all its children.

To overcome this challenge each path $p$, roughly speaking, is replaced by a heap of the edges that have tails on the path from $head(lastedge(p))$ to $t$ and ordered by $\delta(e)$. Then using two intermediate directed acyclic graphs $D(G)$~\cite[Lemma 4]{eppstein1998finding} and $P(G)$~\cite[Lemma 5]{eppstein1998finding}, a heap $H(G)$~\cite[Lemma 6]{eppstein1998finding} is constructed with the following properties:
\begin{itemize}
\item $H(G)$ is a 4-heap;
\item There is a bijection mapping between nodes in $H(G)$ and $s-t$ paths in $G$;
\item The length of an $s-t$ path in $G$ is $d(s,t)$ plus the weight of the corresponding node in $H(G)$.
\end{itemize}

Finding $k$ smallest nodes in a min-heap costs $k\log k$, which can be further improved by Frederickson's technique~\cite{frederickson1993optimal}, and hence the time complexity of the Eppstein's algorithm~\cite[Lemma 7]{eppstein1998finding}.


\section{Lymphoma}
