\chapter{Background}

\section{Machine Learning}
Machine learning techniques are used to extract information from data, or make some predictions about the data. This chapter briefly explains methods and techniques used in, or required to understand the proceeding chapters.

\subsection{Feature Selection}
Feature selection is the task of selecting important features to the problem at hand. It becomes particularly a hard task when the number of features in the data is of a higher magnitude compared to the number of given samples. Table~\ref{tab:sample-sample-size} shows an example number of samples vs. number of features in a typical data. One of the challenges when dealing with such a large number of features is that if enough number of features have a probability distribution independent of the outcome, some of them might falsely seem correlated with the outcome. Another obstacle comes from the fact that our features are not independent and they function in complex networks. As a result, features should be considered in groups, which is a combinatorial and intractable problem.

\begin{table}[th!]
  \centering
  \begin{tabular}{p{3cm} p{3.5cm} p{4cm}}
    \hline
    \multicolumn{3}{c}{Sample Data} \\
    \cline{1-3}
    Sample Count   & Gene Expression Data Feature Count & 450K Methylation Chip Data Feature Count \\
    \hline
    $500$      & $\approx 20,000$    & $\approx 450,000$   \\
    \hline
  \end{tabular}
  \caption{An example number of samples and features in our usual data}
  \label{tab:sample-sample-size}
\end{table}

We have used correlation~\cite{correlation}, mutual information~\cite{mutual-information}, and $l_1$-regularized methods~\cite{l1-regularized} as techniques to select features.

\subsection{Classification}
Classification is the problem of putting data into different classes~\cite{statistical-learning}. During the training phase, the matrix $X_{samples \times features}$ is given as the input and $y_{samples}$ as the desired output. The vector $y$ has values from a discrete set. If the set has only two distinct values, the problem is called a binary classification.

Logistic regression~\cite{logistic-regression1,logistic-regression2}, Support Vector Machines (SVM)~\cite{svm1,svm2}, and decision trees~\cite[TODO:chapter]{statistical-learning} are examples of classification methods.

\subsection{Regression}
In statistics, predicting a continues output value given an input data is called regression~\cite[TODO:chapter]{statistical-learning}. Regression and classification differ in their desired output type. In regression the output is continues in contrast to classification in which the output is a discrete value.

Linear regression~\cite[TODO:chapter]{statistical-learning}, Gaussian processes~\cite{gaussian-processes}, and kernel based regression~\cite[TODO:chapter]{learning-with-kernels} are some available methods here.

\subsection{Regularization}
Building a machine to predict the outcome with a good performance on the training set is easy if the number of features in the data is large enough compared to the number of samples, even if features are drawn from a random background probability distribution independent of the outcome. But the trained machine will perform poorly on the unseen test samples. This phenomenon is called overfitting. One way to prevent overfitting is to select potential features before training a selected model.

Another way to tackle the problem is to reduce the complexity of the models. This can be done via regularization. The $l_1$-regularization is an appropriate tool when the intention is to reduce the number of features a model takes into account for prediction as well as its complexity~\cite{l1-regularized}.

Assume a model minimizes a loss function $E(X, Y)$, where $X$ is the input matrix and $Y$ is the output vector or matrix. In most regression models the loss function is defined as:

\begin{align}
  E(X, Y) = \parallel Y - X \beta \parallel_2
  \label{frm:loss-function}
\end{align}

The optimization algorithm finds a $\beta$ that minimizes the loss function in Formula~\ref{frm:loss-function}. Having enough number of features, the optimization algorithm might find a $\beta$ that gives a perfect loss, i.e. 0. But in noisy environments the resulting $\beta$ is probably not the real $\beta$ of the underlying model producing the data. The vector $\beta$ might also have some extreme values that are likely not desired. Penalizing the \emph{size} of $\beta$ as shown in Formula~\ref{frm:regularized} will address the abovementioned concern. The size of a vector in this context is represented by its $l_1$ or $l_2$ norm as defined in Formula~\ref{frm:lp-norm}.


\begin{align}
  \parallel \beta \parallel_p := \left(\sum\limits_{i=1}^n \mid \beta_i \mid^p \right)^{1/p}
  \label{frm:lp-norm}
\end{align}
\begin{align}
  &E(X, Y) + \alpha \parallel \beta \parallel_2 = \parallel Y - X \beta \parallel_2 + \alpha \parallel \beta \parallel_2 \nonumber \\
  &\text{or} \nonumber \\
  &E(X, Y) + \alpha \parallel \beta \parallel_1 = \parallel Y - X \beta \parallel_2 + \alpha \parallel \beta \parallel_1
  \label{frm:regularized}
\end{align}

\subsection{Empirical Risk Minimization}
We need to minimize the risk of the error of the predictor~\cite[p.~20]{thenatureofstatisticallearningtheory}.

\subsection{Kernel Methods}

\subsection{Gaussian Processes}

\subsection{Boosting and Ensemble Methods}
For a given prediction problem the idea of boosting is to find an optimal combination of classifiers, also called ``weak learners''~\cite{ensemble2002}. There are many methods of finding the optimal combination of such weak learners, two of which are stochastic gradient boosting~\cite{friedman2002stochastic} and AdaBoost~\cite{adaboost97}. Stochastic gradient boosting tries to estimate the gradients of the loss function and train each individual weak learner in a way that best improves the loss function. AdaBoost tries to identify samples among given data samples that are harder to classify, and gives them more weight in the process of training individual weak learners. One way of improving AdaBoost is to take into account the confidences of predictions given by weak learners if possible and use estimated confidences in the voting process~\cite{adaboost99improved}.

\section{K Best Shortest Paths}

\section{Lymphoma}
