\chapter{Background}
\label{sec:background}

This chapter covers the very basics of some of the concepts required to follow the work in later chapters. These concepts are divided into three sections: Section~\ref{sec:bkg:ml} machine learning, Section~\ref{sec:bkg:graph} graph theory, and Section~\ref{sec:bkg:biology} cell biology. This chapter by far does not exhaustively covers explained concepts, and the reader is strongly recommended to study the subjects using the references and text books cited throughout the chapter. In particular, The Elements of Statistical Learning~\cite{statistical-learning}, Pattern Recognition and Machine Learning~\cite{bishop2006pattern}, and Introduction to Graph Theory~\cite{west2001introduction} cover machine learning and graph theory respectively; and the biology and cancer background is best covered by Molecular Biology of the Cell~\cite{the-cell} and Postgraduate Haematology~\cite{hoffbrand2016postgraduate}.

\section{Machine Learning}
\label{sec:bkg:ml}
Machine learning techniques are used to extract information from data, or make some predictions about the data. We can recognize two groups of learning problems: supervised and unsupervised learning. Unsupervised learning, deals with data sets which are in the form of a set of data points, and no desired output is given. Clustering is the most studied unsupervised learning problem which is the task of grouping similar data together in certain clusters~\cite[Ch. 14]{statistical-learning}~\cite[Ch. 1]{murphy2012machine}.

Supervised learning deals with data sets that are in the form of a set of input and outputs, and the task at hand is to predict the output using the input~\cite[Ch. 2]{statistical-learning},~\cite[Ch. 1]{murphy2012machine}. Classification and regression are supervised learning problems. 

Classification is the problem of putting data into different classes~\cite[Ch. 1]{statistical-learning}. During the training phase, the matrix $X_{samples \times features}$ is given as the input and $y_{samples}$ as the desired output. The vector $y$ has values from a discrete set. If the set has only two distinct values, the problem is called a binary classification. On the other hand, if the output is a continues variable, then it is called a regression problem~\cite[Ch. 1]{statistical-learning}.

Logistic regression~\cite{logistic-regression1,logistic-regression2}, Support Vector Machines (SVM)~\cite{svm1,svm2}, and decision trees~\cite[Ch. 9]{statistical-learning} are examples of classification methods, and Linear regression~\cite[Ch. 3]{statistical-learning}, Gaussian processes~\cite{gaussian-processes}, and kernel based regression~\cite[Ch. 9]{learning-with-kernels} are some well established regression methods.

\subsection{Empirical Risk Minimization}
\label{chap:empirical-risk-minimization}
As mentioned above, supervised learning deals with predicting an output $y_i \in Y$ given an input $x_i \in X$. The task is to find a function $f(.)$ which best predicts the output, given any possible input. Because in practice we have access to a limited given data set, the best we can do is to find a function that best predicts the output given any input in the data set.

We can formulate finding the best function $f(.)$, as finding a function that has the minimum loss over the data. Therefore we need a loss function defined as $L(x, y, f(x))$, with $x$ being the input, $y$ the desired output, and $f(x)$ the predicted output. The loss value has to be in $[0, \infty)$, and $L(x, y, y) = 0$~\cite[p. 62]{learning-with-kernels}. The empirical risk function is then defined as~\cite[p. 67]{learning-with-kernels}:

  \begin{align}
    R_{emp}[f] := \frac{1}{m}\sum_{i = 1}^{m} L(x_i, y_i, f(x_i))
  \end{align}
  
Now let $\mathcal{F}$ be the function space available to choose $f(.)$ from it. The best function is one which minimizes the risk function~\cite[p. 67]{learning-with-kernels}:

\begin{align}
  \arg \min_{f \in \mathcal{F}} R_{emp}[f] = \arg \min_{f \in \mathcal{F}} \frac{1}{m}\sum_{i = 1}^{m} L(x_i, y_i, f(x_i))
\end{align}

But there is a problem with the above formulation if the function space $\mathcal{F}$ is rich enough to fit to the given data too well. Imagine a function that returns $y_i$ for each $x_i$ in the training set, and $0$ otherwise. This function clearly has a minimum loss of $0$, but does not generalize on unseen data. This is called overfitting in machine learning. One way to fix this issue is to regularize the loss function in some way, and give preference to functions $f(.)$ with lower complexity, or smoother functions. This is referred to as regularized empirical risk minimization~\cite[Ch. 4.1]{learning-with-kernels}, or structural risk minimization~\cite[Ch. 4.1]{thenatureofstatisticallearningtheory}. Assume $\Omega(f)$ is a penalty assigned to function $f(.)$; then the regularized empirical risk is formulated as:

\begin{align}
  R_{emp}[f] := \frac{1}{m}\sum_{i = 1}^{m} L(x_i, y_i, f(x_i)) + \lambda \Omega(f)
  \label{frm:bkg:emp-regularized}
\end{align}

Parameter $\lambda$ is the regularization term and we estimate it, among other model parameters, using cross validation.

For instance, in the case of a linear model, assume a model minimizes a loss function $E(X, Y)$, where $X$ is the input matrix and $Y$ is the output vector or matrix, defined as:

\begin{align}
  E(X, Y) = \parallel Y - X \beta \parallel_2
  \label{frm:loss-function}
\end{align}

The optimization algorithm finds a $\beta$ that minimizes the loss function in Formula~\ref{frm:loss-function}. As explained above, having enough number of features, the optimization algorithm might find a $\beta$ that gives a perfect loss, \emph{i.e.} 0. But in noisy environments the resulting $\beta$ is probably not the real $\beta$ of the underlying model producing the data. The vector $\beta$ might also have some extreme values that are likely not desired. Penalizing the \emph{size} of $\beta$ as shown in Formula~\ref{frm:regularized} will address the abovementioned concern. The size of a vector in this context is represented by its $l_1$ or $l_2$ norm as defined in Formula~\ref{frm:lp-norm}. The $l_1$-regularization is an appropriate tool when the intention is to reduce the number of features a model takes into account for prediction as well as its complexity~\cite{l1-regularized}, since it favors more absolute zeros in the $\beta$ vector, and hence is also used as a feature selection tool.

\begin{align}
  \parallel \beta \parallel_p := \left(\sum\limits_{i=1}^n \mid \beta_i \mid^p \right)^{1/p}
  \label{frm:lp-norm}
\end{align}
\begin{align}
  &E(X, Y) + \alpha \parallel \beta \parallel_2 = \parallel Y - X \beta \parallel_2 + \alpha \parallel \beta \parallel_2 \nonumber \\
  &\text{or} \nonumber \\
  &E(X, Y) + \alpha \parallel \beta \parallel_1 = \parallel Y - X \beta \parallel_2 + \alpha \parallel \beta \parallel_1
  \label{frm:regularized}
\end{align}

\subsection{Cross Validation}
\label{chap:cross-validation}

Cross validation is a technique used in method selection and performance estimation. In cross validation we divide the given training data into $k$ folds, set aside one of those $k$ folds, train the model on $k - 1$ remaining sections, and test the performance of the model on the set aside part of the data. Then repeat this process for all $k$ folds to assess the overall performance of the method. A special case of $k$-fold cross validation is leave-one-out in which $k=n$, the number of samples. Leave-one-out cross validation is computationally intensive for relatively large number of samples. A popular $k$ is 10, which is shown to have lower variance than leave-one-out method, and it has a low bias~\cite[Ch. 7]{statistical-learning}. 

There are also some variations to the simple $k$-fold cross validation scheme. One way is to repeat the $k$-fold system multiple times with a random shuffle of the data before each $k$-fold test, and calculate the estimated error using the repeated test. Another variation is to randomly partition the data into train and test partitions several times and use these sets to estimate the performance of the method.

The latter two variations are shown to give better estimates of the true error of the method compared to leave one out and a single $10$-fold scheme~\cite{kim2009estimating, efron1994introduction}. Because repeating a $k$-fold scheme can be computationally intensive depending on the method being tested, we sometimes use a repeated random partitioning of the data in our work. In some even more computationally intensive cases, we have limited our analysis to a single $k$-fold scheme to select methods.
  
\subsection{Feature Selection}
Feature selection is the task of selecting features most relevant and predictive to the problem at hand from the given set of features. It becomes particularly a hard task when the number of features in the data is of a higher magnitude compared to the number of given samples. Table~\ref{tab:sample-sample-size} shows an example number of samples vs. number of features in a typical data in this study. One of the challenges when dealing with such a large number of features is that if there are enough number of features, even if they have a probability distribution independent of the outcome, some of them might falsely seem correlated with the outcome due to the relatively small sample size. Another obstacle comes from the fact that our features are not independent and they function in complex networks. As a result, features should be considered in groups, which is a combinatorial and intractable problem.

\begin{table}[th!]
  \centering
  \begin{tabular}{p{.2\textwidth} p{.3\textwidth} p{.38\textwidth}}
    \hline
    \multicolumn{3}{c}{Sample Data} \\
    \cline{1-3}
    Sample Count   & Gene Expression Data Feature Count & 450K Methylation Chip Data Feature Count \\
    \hline
    $500$      & $\approx 20,000$    & $\approx 450,000$   \\
    \hline
  \end{tabular}
  \caption{An example number of samples and features in our usual data}
  \label{tab:sample-sample-size}
\end{table}

We have used correlation~\cite{correlation}, mutual information~\cite{mutual-information}, and $l_1$-regularized methods~\cite{l1-regularized} as techniques to select features.

\subsection{Support Vector Machines}
\label{sec:svms}

Support vector machines (SVM) can be used both for regression and classification tasks~\cite{svm2, svr1}. As a binary classifier, SVM finds an optimal hyperplane to separate data points in the feature space by maximizing the hyperplane's margin to the nearest points on both sides of it. Therefore given a test data point, its side with regard to the hyperplane determines its class. As a regressor however, SVM finds an optimal hyperplane to interpolate given data points by minimizing the hyperplane's distance from data points. In this work we use SVMs as a binary classifier.

Formally speaking, given a data-set $\mathcal{D}$ of $n$ data points:

\begin{align}
  \mathcal{D}={(\mathbf{x}_i, y_i) | \mathbf{x}_i \in \mathbb{R}^p, y_i \in {-1, 1}}_{i=1}^{n}
\end{align}

where $\mathbf{x}_i$ is a real vector of length $p$, and $y_i$ is either $1$ or $-1$. A $p$-dimensional hyperplane, characterized by its normal vector $\mathbf{w}$ and its intercept $\mathbf{b}$, is the set of points $\mathbf{x}$ that fit in Formula~\ref{frm:hyperplane}.

\begin{align}
  \mathbf{w} . \mathbf{x} - b = 0
  \label{frm:hyperplane}
\end{align}

Now consider two hyperplanes on both sides of the abovementioned hyperplane as formulated bellow:

\begin{align}
  \mathbf{w} . \mathbf{x} - b &= 1 \nonumber \\
  \mathbf{w} . \mathbf{x} - b &= -1
\end{align}

The distance between each of these hyperplanes and the one in the middle is $\frac{1}{\parallel \mathbf{w} \parallel}$. Therefore the distance between the two of them is $\frac{2}{\parallel \mathbf{w} \parallel}$. For now we assume the data is linearly separable in its feature space, \emph{i.e.} there exists a hyperplane that perfectly separates the data into two classes without error. Such a hyperplane satisfies the following constraint:

\begin{align}
  y_i (\mathbf{w} . \mathbf{x}_i - b)\geq 1 \text{ for all } 1 \leq i \leq n
\end{align}

An optimal hyperplane is one such that it maximizes the margin; hence formulated as Formula~\ref{frm:svm2}. An illustration of the optimal solution is presented in Fig.~\ref{fig:svm-hyperplane}.

\begin{align}
  &\arg\max_{(\mathbf{w},b)}\frac{1}{\|\mathbf{w}\|_2} \nonumber \\
  &\text{s.t.} \nonumber \\
  &y_i (\mathbf{w} . \mathbf{x}_i - b)\geq 1 \text{ for all } 1 \leq i \leq n
  \label{frm:svm2}
\end{align}

\begin{figure}[!ht]
  \centering
  \includegraphics[width=.5\textwidth]{figs/background/Svm_max_sep_hyperplane_with_margin}
  \caption{Illustration of the optimal hyperplane in a support vector machine model, for a 2-dimensional data.}
  \label{fig:svm-hyperplane}
\end{figure}

However, for an easier optimization and mathematical convenience, the above optimization problem is usually formulated as Formula~\ref{frm:svm1} which has the same solution as $\mathbf{w}$ and $b$~\cite[Ch. 5]{thenatureofstatisticallearningtheory},~\cite[Ch. 7]{learning-with-kernels}.

\begin{align}
  &\arg\min_{(\mathbf{w},b)}\frac{1}{2}\|\mathbf{w}\|_2^2 \nonumber \\
  &\text{s.t.} \nonumber \\
  &y_i (\mathbf{w} . \mathbf{x}_i - b)\geq 1 \text{ for all } 1 \leq i \leq n
  \label{frm:svm1}
\end{align}

which can be written as Formula~\ref{frm:svm3} after introducing Karush-Kuhn-Tucker (KKT) multipliers~\cite{kkt-orig}~\cite[Ch. 5]{thenatureofstatisticallearningtheory}.

\begin{align}
  &\arg\min_{\mathbf{w},b } \max_{\boldsymbol{\alpha}} \left\{ \frac{1}{2}\|\mathbf{w}\|_2^2 - \sum_{i=1}^{n}{\alpha_i[y_i(\mathbf{w}\cdot \mathbf{x_i} - b)-1]} \right\} \nonumber \\
  &\text{s.t.} \nonumber \\
  &\alpha_i \geq 0 \text{ for } 1 \leq i \leq n \nonumber \\
  \label{frm:svm3}
\end{align}

Multipliers $\alpha_i$ will be $0$ for each $\mathbf{x}_i$ that does not lie on either of the marginal hyperplanes. For example in Figure~\ref{fig:svm-hyperplane}, $\alpha_i$ is non-zero for only three of the data points; the ones that are exactly on either of the marginal lines. The corresponding $\mathbf{x}_i$ for which $\alpha_i$ is non-zero are called \emph{support vectors}.

It can be shown that Formula~\ref{frm:svm-dual1} is a dual of the optimization problem defined in Formula~\ref{frm:svm3}~\cite[p. 14]{learning-with-kernels}~\cite[Ch. 5]{thenatureofstatisticallearningtheory}.

\begin{align}
  &\arg\max_{\boldsymbol{\alpha}}\left\{\sum_{i=1}^n \alpha_i - \frac{1}{2}\sum_{i, j} \alpha_i \alpha_j y_i y_j \mathbf{x}_i^T \mathbf{x}_j\right\} \nonumber \\
  &\text{s.t.} \nonumber \\
  &\alpha_i \geq 0 \text{ for } 1 \leq i \leq n \nonumber \\
  &\sum_{i=1}^{n}\alpha_i y_i = 0
  \label{frm:svm-dual1}
\end{align}

Now assume the following notations and definitions:

\begin{align}
  \phi(\mathbf{x}) &:= \mathbf{x} \nonumber \\
  \langle \mathbf{x}_i, \mathbf{x}_j \rangle &:= \mathbf{x}_i^T \mathbf{x}_j \nonumber \\
  k(\mathbf{x}_i, \mathbf{x}_j) &:= \langle \phi(\mathbf{x}_i), \phi(\mathbf{x}_j) \rangle
  \label{frm:svm-ktrick1}
\end{align}

Putting function $k$ in Formula~\ref{frm:svm-dual1}, the SVM's optimization problem can be written as:

\begin{align}
  &\arg\max_{\boldsymbol{\alpha}}\left\{\sum_{i=1}^n \alpha_i - \frac{1}{2}\sum_{i, j} \alpha_i \alpha_j y_i y_j k(\mathbf{x}_i, \mathbf{x}_j)\right\} \nonumber \\
  &\text{s.t.} \nonumber \\
  &\alpha_i \geq 0 \text{ for } 1 \leq i \leq n \nonumber \\
  &\sum_{i=1}^{n}\alpha_i y_i = 0
  \label{frm:svm-dual2}
\end{align}

The identity function used in Formula~\ref{frm:svm-ktrick1} is not the only option. We can transform the data into another feature space using a different $\phi(.)$, and then use dot-product in that space. This is useful for cases that the data is not linearly separable in its original feature space, but linearly separable using a non-linear transformation.

Using Mercer's theorem~\cite{mercer-theorem} and its corollary Mercer's condition, it can be shown that any function $k$ satisfying the following condition can be used as a \emph{kernel} in Formula~\ref{frm:svm-dual2}~\cite[Ch. 2.2]{learning-with-kernels}.

\begin{align}
  \forall \mathcal{D}, \forall c_i, c_j \in \mathbb{R}: \sum_{i=1}^n\sum_{j=1}^n k(\mathbf{x}_i, \mathbf{x}_j) c_i c_j \geq 0
  \label{frm:positive-definite}
\end{align}

This means to solve the SVM optimization, we only need the kernel matrix $\mathbf{K}$, which has $k(\mathbf{x}_i, \mathbf{x}_j)$ as its values. For many kernels, this matrix can be calculated directly without transforming the data into the alternate representing feature space. This also means that we can use kernels which have an infinite dimensional representing feature space. This technique is sometimes called the \emph{kernel trick}. Arguably, other than dot-product, the most famous kernel function $k$ satisfying the above condition is the \emph{Gaussian kernel}, also known as the \emph{radial basis function (RBF) kernel}~\cite[Ch. 2]{learning-with-kernels}:

\begin{align}
  &k(\mathbf{x_i}, \mathbf{x_j}) = \exp\left(-\frac{||\mathbf{x_i} - \mathbf{x_j}||^2}{2\sigma^2}\right) \nonumber \\
  &\sigma \in \mathbb{R}
  \label{frm:rbf-kernel1}
\end{align}

This is an example of a kernel with an infinite dimensional representing kernel space~\cite{steinwart2006explicit}. Many implementations use a different formulation which uses a different parametrization, using $\gamma=\frac{1}{2\sigma^2}$:

\begin{align}
  &k(\mathbf{x_i}, \mathbf{x_j}) = \exp\left(-\gamma||\mathbf{x_i} - \mathbf{x_j}||^2\right) \nonumber \\
  &\gamma \in \mathbb{R}^+
  \label{frm:rbf-kernel2}
\end{align}

\subsubsection{Regularization of Support Vector Machines}
In real-world applications data-sets are often not linearly separable, \emph{i.e.} no hyperplane can perfectly separate the two classes of the data-set. To handle such cases, Formula~\ref{frm:svm1} can be modified as Formula~\ref{frm:svm-slack} with the introduction of $\xi_i$ called slack variables~\cite{cortes1995support},\cite[Ch. 7.5]{learning-with-kernels}. This allows some of the data points to be within the margin area or to be on the wrong side of the hyperplane. This formulation is also referred to as a soft margin hyperplane.

\begin{align}
  &\arg\min_{(\mathbf{w},b)}\frac{1}{2}\|\mathbf{w}\|_2^2 + C \sum_{i=1}^n \xi_i \nonumber \\
  &\text{s.t.} \nonumber \\
  &y_i (\mathbf{w} . \mathbf{x}_i - b)\geq 1 - \xi_i \text{ for all } 1 \leq i \leq n
  \label{frm:svm-slack}
\end{align}

A formula similar to Formula~\ref{frm:bkg:emp-regularized} can be derived from Formula~\ref{frm:svm-slack} as shown in Formula~\ref{frm:bkg:svm-lambda-l2}~\cite[Ch. 12]{statistical-learning}~\cite{hastie2004entire}.

\begin{align}
  \arg\min_{(\mathbf{w},b)}\sum_{i=1}^{n}[1-y_i(\mathbf{w}x_i - b)] + \frac{\lambda}{2}\|\mathbf{w}\|_2^2
  \label{frm:bkg:svm-lambda-l2}
\end{align}

Note that parameter $C$ in Formula~\ref{frm:svm-slack} corresponds to $\frac{1}{\lambda}$ in Formula~\ref{frm:bkg:svm-lambda-l2}. The corresponding penalty function of Formula~\ref{frm:bkg:emp-regularized} in Formula~\ref{frm:bkg:svm-lambda-l2} is the $l^2$-norm of the vector $\mathbf{w}$. Similar to methods such as \emph{lasso}~\cite[Ch. 3]{statistical-learning} this penalty function can be replaced with the $l^1$-norm of the vector $\mathbf{w}$ shown in Formula~\ref{frm:bkg:svm-lambda-l1}~\cite{zhu20041}.

\begin{align}
  \arg\min_{(\mathbf{w},b)}\sum_{i=1}^{n}[1-y_i(\mathbf{w}x_i - b)] + \frac{\lambda}{2}\|\mathbf{w}\|_1^2
  \label{frm:bkg:svm-lambda-l1}
\end{align}

It is important to note that the algorithm to solve the above optimization problem does not involve the kernel trick~\cite{zhu20041}, which means we cannot use similarity measures that require transforming data into spaces we cannot compute, such as the RBF kernel. In order to use the $l^1$-norm regularized SVM with such kernels, an approximation of the feature space can be used to transform the data first, and then apply the above optimization problem on the transformed data~\cite{rahimi2007random}.

\subsection{Gaussian Processes}
Given a regression or a classification problem, one approach is to find the most likely function among the functions we consider reasonable for our problem. For instance, a linear regression assumes the underlying function explaining the data to be linear, and then tries to find one which is most probable to be the real underlying function for the given data. Another example are support vector machines for a classification problem, which try to find the best separating hyperplane, \emph{i.e.} a linear function, and assume that function explains the data the best.

An alternative approach is to consider all available functions at the same time, and assign a probability to each function according to how well they explain the data. To illustrate the idea better, assume we have a family of functions as our prior (Figure~\ref{fig:bkg:gp-intro}(a)), and then we observe a few values from the underlying function. If the data is noiseless, only those functions passing all of our observations can be considered (Figure~\ref{fig:bkg:gp-intro}(b)). Considering all those functions, we can calculate a posterior mean and variance for each unobserved value as shown in Figure~\ref{fig:bkg:gp-intro}(c)~\cite{gaussian-processes}.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=1\textwidth]{figs/background/Gaussian_Process_Regression}
  \caption[(a) Samples from prior family of functions, (b) samples from posterior family of functions, and (c) predicted mean and variance]{(a) Samples from prior family of functions, (b) samples from posterior family of functions, and (c) predicted mean and variance of the posterior\protect\footnotemark.}
  \label{fig:bkg:gp-intro}
\end{figure}
\footnotetext{Image by Cdipaolo96 (\url{https://en.wikipedia.org/wiki/Gaussian_process}) licensed under CC BY-SA 4.0.}

Gaussian processes are particularly useful if not only the mean of the prediction is of interest, but also an estimate of the variance of the posterior probability distribution is important, which is the case for us as explained in section~\ref{chap:ratboost-chapter}.

To formulate the above intuition, consider a regression problem, with some observed inputs $x_i$ and corresponding outputs $y_i$. The goal is usually to find the best function $f$ from a given family of functions such as linear functions, for which $y_i = f(x_i)$. Alternatively, we could infer a distribution over functions given the data, \emph{i.e.} $p(f|\mathbf{X}, \mathbf{y})$, and give predictions for a new input $\mathbf{x}_*$ as shown in formula~\ref{frm:bkg:gp-1}~\cite{murphy2012machine}.

\begin{align}
  p(y_*|x_*,\mathbf{X}, \mathbf{y}) = \int p(y_*|f, \mathbf{x_*})p(f|\mathbf{X}, \mathbf{y})df
  \label{frm:bkg:gp-1}
\end{align}

Fortunately, it turns out given a finite input dataset, predicting the mean and variance of the output given a new input can be done without having to compute the above integral. To explain, here we follow the path in \emph{Pattern Recognition and Machine Learning, Bishop}~\cite{bishop2006pattern}. Similar to SVMs, consider $\phi(\mathbf{x})$ to be the transformation function for input $\mathbf{x}$, and the following linear model:

\begin{align}
  y(\mathbf{x}) = \mathbf{w}^T \phi(\mathbf{x})
  \label{frm:bkg:gp-2}
\end{align}

Now assume a Gaussian distribution over the weight vector $\mathbf{w}$ as Formula~\ref{frm:bkg:gp-3}, in which $\alpha$ is the inverse variance of the distribution.

\begin{align}
  p(\mathbf{w}) = \mathcal{N}(\mathbf{w}|\mathbf{0}, \alpha^{-1}\mathbf{I})
  \label{frm:bkg:gp-3}
\end{align}

Any sample taken from the above distribution represents a function in Formula~\ref{frm:bkg:gp-2}, hence the above distribution defines a distribution over linear functions. Now we are interested in evaluating the function on training data $\mathbf{x}_{\{1\ldots N\}}$, \emph{i.e.} function values $y(\mathbf{x}_{\{1\ldots N\}})$ denoted by the vector $\mathbf{y}$, written as:

\begin{align}
  \mathbf{y} = &\Phi\mathbf{w} \nonumber \\
  \Phi_{nk} = &\phi_k(\mathbf{x}_n)
  \label{frm:bkg:gp-4}
\end{align}

The probability distribution of $\mathbf{y}$ is Gaussian since it is a linear combination of elements of $\mathbf{w}$, which are Gaussian distributed. Hence we have:

\begin{align}
  \mathbb{E}[\mathbf{y}] & = \Phi\mathbb{E}[\mathbf{w}] = 0 \nonumber \\
  cov[\mathbf{y}] & = \mathbb{E}[\mathbf{yy}^T] = \Phi\mathbb{E}[\mathbf{ww}^T]\Phi^T = \alpha^{-1}\Phi\Phi^T = \mathbf{K} \nonumber \\
  \mathbf{K}_{mn} & = k(\mathbf{x}_n,\mathbf{x}_m) = \alpha^{-1}\phi(\mathbf{x}_n)^T\phi(\mathbf{x}_m)
  \label{frm:bkg:gp-5}
\end{align}

Similar to SVMs, $k(\mathbf{x},\mathbf{x}')$ is called the kernel function. In general, a Gaussian process is a distribution over functions $y(\mathbf{x})$ such that the join distribution of $y(\mathbf{x}_{1\ldots N})$ is Gaussian for any input set $\mathbf{x}_{1\ldots N}$. Since the joint distribution can be specified using the second order statistics, \emph{i.e.} the mean and the covariance of the distribution, a Gaussian process is completely specified given the two statistics. In many applications we do not have a prior knowledge about the mean of $y(\mathbf{x})$ and by symmetry we assume it to be $0$. Therefore specification of the covariance function would be the only requirement, which itself is given by the kernel function:

\begin{align}
  \mathbb{E}[y(\mathbf{x}_n)y(\mathbf{x}_m)] = k(\mathbf{x}_n,\mathbf{x}_m)
  \label{frm:bkg:gp-6}
\end{align}

The kernel function defined in Formula~\ref{frm:bkg:gp-5} specifies a Gaussian process defined by a linear regression. Two other commonly used kernel functions are Gaussian kernel defined in Formula~\ref{frm:rbf-kernel1} and exponential kernels defined as:

\begin{align}
  k(\mathbf{x}_n,\mathbf{x}_m) = exp(-\theta |x - x'|)
  \label{frm:bkg:gp-6-2}
\end{align}

Now given a new test data $\mathbf{x}_{N+1}$, we calculate the kernel matrix and partition it as shown in Formula~\ref{frm:bkg:gp-7}.

\begin{align}
  \mathbf{K}_{N+1} =
  \begin{pmatrix}
    \mathbf{K}_N & \mathbf{k} \\
    \mathbf{k}^T & c
  \end{pmatrix}
  \label{frm:bkg:gp-7}
\end{align}

Then, as shown in \emph{Bishop, 2006}~\cite[Ch. 6.4]{bishop2006pattern}, the predicted mean and variance for the input would be:

\begin{align}
  m(\mathbf{x}_{N+1}) & = \mathbf{k}^T\mathbf{K}_N^{-1}\mathbf{y} \nonumber \\
  \sigma^2(\mathbf{x}_{N+1}) & = c-\mathbf{k}^T\mathbf{K}_N^{-1}\mathbf{k}
  \label{frm:bkg:gp-8}
\end{align}


\subsection{Boosting and Ensemble Methods}
For a given prediction problem the idea of boosting is to find an optimal combination of classifiers, also called ``weak learners'', in such a way that a combination of their outputs improves the prediction accuracy. This combination can be majority voting, average, or a weighted average of the outputs of the weak learners~\cite{ensemble2002}. There are many methods of finding the optimal combination of such weak learners, two of which are stochastic gradient boosting~\cite{friedman2002stochastic} and AdaBoost~\cite{adaboost97}. Stochastic gradient boosting tries to estimate the gradients of the loss function and train each individual weak learner in a way that best improves the loss function. AdaBoost tries to identify samples among given data samples that are harder to classify, and gives them more weight in the process of training individual weak learners. One way of improving AdaBoost is to take into account the confidences of predictions given by weak learners if possible and use estimated confidences in the voting process~\cite{adaboost99improved}.

\section{Shortest Path Algortihms for Graphs}
\label{sec:bkg:graph}
A graph $G$ is a set of vertices (also called nodes) $V$, and a set of edges $E=\left\{(v_i, v_j) | v_i, v_j \in V\right\}$ that connect vertices in $V$. A graph can be directed or undirected. In directed graphs, edges have direction, \emph{i.e.} edge $(s, t)$ is different than the edge $(t, s)$. In other words, the following list shows the possible sets of edges regarding vertices $s$ and $t$ in a directed graph:

\begin{align}
  E &= \{\} \nonumber \\
  E &= \{(s, t)\} \nonumber \\
  E &= \{(t, s)\} \nonumber \\
  E &= \{(s, t), (t, s)\}
\end{align}

In an undirected graph however, edges $(s, t)$ and $(t, s)$ are identical, and can in fact be represented as a set $\{s, t\}$ instead of an ordered pair.

Graphs can also be weighted or not. If a graph $G$ is weighted, then there is a weight assigned to each edge of the graph. We use $w_{s,t}$ to note the weight of the edge $(s,t)$. In undirected graphs, $w_{s, t}$ is always the same as $w_{t, s}$. Sometimes the weight of an edge is referred to as the length of an edge and noted as $l_{s,t}$ depending on the context in the literature. A sequence of $n$ nodes $(v_1, v_2,\ldots,v_n) \in V^n$ defines a path $p$ of length $n$ if for every consecutive nodes $v_i$ and $v_{i+1}$, $(v_i, v_{i+1})$ is an edge in the graph. The weight or the length of a given path is the sum over the corresponding weights/lengths of its edges.
An undirected graph $G$ is connected if there is at least one path between every given two vertices on the graph.

\subsection{The Shortest Path Problem}
\emph{The shortest path problem} is to find a path between two vertices $s$ and $t$ such that the total weight of the path is the minimum among all possible paths between the two nodes. In unweighted graphs, the weight of each edge is considered to be $1$. Figure~\ref{fig:shortest-path1} highlights the shortest path between vertices $A$ and $F$ on the given weighted directed graph.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.5\textwidth]{figs/background/Shortest_path_with_direct_weights}
  \caption{A given weighted directed graph and the highlighted shortest path between vertices $A$ and $F$.}
  \label{fig:shortest-path1}
\end{figure}

Some prominent algorithms to solve the shortest path problem are Dijkstra's~\cite{dijkstra}, Bellman-Ford~\cite{bellman-ford}, and Floyd-Warshall~\cite{floyd-warshall} algorithms. Dijkstra's algorithm applies to the single-source shortest path problem on graphs with non-negative weight values with time complexity $O(|E| + |V| \log |V|)$~\cite{fredman1987fibonacci}, whereas Bellman-Ford algorithm works on graphs with also negative weights having time complexity $O(|V| |E|)$. Floyd-Warshall algorithm, on the other hand, solves the all pairs shortest path problem, \emph{i.e.} it finds shortest paths between all pairs of vertices, on graphs with negative and non-negative weight values and its time complexity is $O(|V|^3)$. The big $O$ notation defines an asymptotically upper bound of a function up to a constant factor. Formula \ref{frm:bkg:big-O} formally defines the notation \cite{thomas2001introduction}. 

\begin{align}
  f(x) = O(g(x)) \iff \exists k > 0 \exists n_0 \forall n > n_0 |f(n)| \leq k|g(n)|
  \label{frm:bkg:big-O}
\end{align}


\subsection{The $k$ Shortest Paths Problem}
\emph{The $k$ shortest paths problem} is to find the $k$ paths from $s$ to $t$ with minimum weight among all distinct possible paths from $s$ to $t$. Whether or not loops in the paths are allowed results in two different definitions of this problem.For the case when the goal is to find $k$ best shortest paths from a single source to all other nodes, Jin Y. Yen published an algorithm of the time complexity $O(k |V|(|E|+|V|\log|V|))$ in 1971 for the loopless setting which still has the best available time complexity~\cite{yen1971finding}. It is possible to achieve better worst case time complexity if we allow loops in the paths. In 1998 Eppstein came up with an algorithm with $O(|E| + |V|\log|V| + |V| k)$ time complexity, and $O(|E| + |V|\log|V| + k)$ if the problem is reduced to the single source single destination case~\cite{eppstein1998finding}. There has been improvements to Eppstein's algorithm, but the worst case time complexity has not been improved.

In our case, the graph is a directed acyclic graph (DAG), \emph{i.e.} there are no directed loops in the graph. Therefore despite we require paths to be loopless, Eppstein's algorithm is sufficient and gives desirable paths.
Intuitively, the algorithm starts with the shortest path between $s$ and $t$, and in each iteration it finds the next shortest path by modifying a part of the previous path. This is achieved by storing a tree of all shortest paths to the destination $t$, then calculating the cost of jumping from one shortest path to another one using edges that are not a part of that tree (called sidetracks), and at the end picking the sidetrack edges with the least costs.
Here we give an overview of the algorithm and postpone our use case in detail to Chapter~\ref{sec:fcs}.

First we need to introduce some concepts and notations, and for the sake of easier reference to the original work, we keep the notation as the work done by Eppstein.
Assume the problem is to find the $k$ shortest paths from $s$ to $t$ on a connected directed graph $G$. Then consider the following:
\begin{itemize}
\item $T$: a single destination shortest path tree with destination $t$, \emph{i.e.} $T$ includes all vertices of $G$ and a shortest path from each node to $t$.
\item $d(v_i, v_j)$: the weight of a shortest path from $v_i$ to $v_j$, or in other words the distance between the two vertices.
\item $head(e), tail(e)$: if $e$ is $(v_i, v_j)$, $head$ and $tail$ of $e$ are $v_i$ and $v_j$ respectively.
\item $l(e)$: weight or length of edge $e$.
\item $\delta(e)$: intuitively the cost of including $e$ in a shortest path to $t$, defined as:
  \begin{align}
    \delta(e) = l(e) + d(head(e), t) - d(tail(e), t)
  \end{align}
\end{itemize}

If the edge $e$ is not a part of $T$, it is a \emph{sidetrack} and the cost of including it in a path to $t$ is non-negative~\cite[Lemma 1]{eppstein1998finding}.

A key point to understanding the algorithm is the way paths are represented. A path $p$ from $s$ to $t$ can be represented by the list of \emph{sidetrack} edges it includes. If the path $p$ includes only one \emph{sidetrack} edge $(v_i, v_j)$, it means the path is the shortest path from $s$ to $v_i$, then the edge $(v_i, v_j)$, and then the shortest path from $v_j$ to $t$. The set $sidetracks(p)$ includes all edges in $p$ that are not in the shortest path tree $T$, \emph{i.e.} they are in $G - T$. The graph $G - T$ is defined as the graph $G$ excluding edges that are present in the graph $T$.

To calculate the length of the path $p$ we have~\cite[Lemma 2]{eppstein1998finding}:
\begin{align}
  l(p) = d(s, t) + \sum_{e \in sidetracks(p)} \delta(e)
\end{align}

Given a path $p$, let $S = sidetracks(p)$ be the sequence of edges of $p$ that are in $G - T$. We also define $path(S)$ as the function calculating the path $p$ from a given $S$. Next, we define $prefix(S)$ to be the sequence of edges in $S$ except the last one. Therefore $prefix(S)$ can define a path as $prefpath(p) := path(prefix(S))$.

Next we have: if the path $p$ is from $s$ to $t$ in $G$ and has a nonempty $sidetracks(p)$, then $l(p) \geq l(prefpath(p))$~\cite[Lemma 3]{eppstein1998finding}. Please note that $sidetracks(p)$ has to be nonempty or else $prefix(S)$ and hence $prefpath(p)$ is undefined. As a corollary of Lemma 2 and 3 we can construct a natural tree of paths which is also a heap style tree. It is a tree in a way that each node is a path $p$, and it has all possible paths $p'$ for which $prefpath(p') = path(p)$. It is also a heap style tree in a way that the length of a parent node is less than or equal to all its children.

To overcome this challenge each path $p$, roughly speaking, is replaced by a heap of the edges that have tails on the path from $head(lastedge(p))$ to $t$ and ordered by $\delta(e)$. Then using two intermediate directed acyclic graphs $D(G)$~\cite[Lemma 4]{eppstein1998finding} and $P(G)$~\cite[Lemma 5]{eppstein1998finding}, a heap $H(G)$~\cite[Lemma 6]{eppstein1998finding} is constructed with the following properties:
\begin{itemize}
\item $H(G)$ is a 4-heap;
\item There is a bijection mapping between nodes in $H(G)$ and $s-t$ paths in $G$;
\item The length of an $s-t$ path in $G$ is $d(s,t)$ plus the weight of the corresponding node in $H(G)$.
\end{itemize}

Finding $k$ smallest nodes in a min-heap costs $O(k\log k)$, which can be further improved by Frederickson's technique~\cite{frederickson1993optimal}, and hence the time complexity of the Eppstein's algorithm~\cite[Lemma 7]{eppstein1998finding}.

Although Eppstein's algorithm has the best know worst-case time complexity, it can be shown that in practice we can achieve faster running times by constructing some parts of the algorithm's intermediate structures as they're needed. In 2003 V{\'\i}ctor M. Jim{\'e}nez and Andr{\'e}s Marzal published the modified version of the algorithm and a more detailed explanation of Eppstein's algorithm~\cite{jimenez2003lazy}.

\section{Cell Biology}
\label{sec:bkg:biology}
In order to understand cancer, we need some basics of cellular molecular biology, most importantly the central dogma of molecular biology which shows how information is transferred and transformed inside cells. The central dogma deals with three types of molecules: Deoxyribonucleic acid (DNA), Ribonucleic acid (RNA), and protein. The flow of information between these three types of molecules is depicted in Figure~\ref{fig:bkg:central-dogma}.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.5\textwidth]{figs/background/Centraldogma_nodetails}
  \caption{Flow of information in biological cells. Blue arrows show the usual flow, and the red arrows show the flow in some special cases.}
  \label{fig:bkg:central-dogma}
\end{figure}

Although most of the explanations in this section apply to all cellular organisms, for the sake of simplicity we focus on multicellular eukariotic organisms, \emph{i.e.} we assume cells have a nucleus and organisms have organs. In this section we cover a minimal background required to explain and understand the basics of the biology of cancer. For an extensive explanation of these topics refer to \emph{"Molecular Biology of the Cell, alberts, et al."}\cite{the-cell}.

\subsection{Deoxyribonucleic acid (DNA)}
DNA molecules are polymers, mostly made of four different unit types called nucleotides: pyrimidines (thymine (T), cytosine (C)) and purines (adenine (A) and guanine (G)). DNA is usually in the form of a double stranded helix, and the two strands complement each other, \emph{i.e.} \emph{T} complements \emph{A} and \emph{C} complements \emph{G} (Figure~\ref{fig:bkg:dna-basic}).

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.6\textwidth]{figs/background/simplified_DNA_Structure_Key_Labelled_pn_NoBB}
  \caption{DNA double helix and base pairs\protect\footnotemark.}
  \label{fig:bkg:dna-basic}
\end{figure}
\footnotetext{Image by Zephyris (\url{https://en.wikipedia.org/wiki/DNA}) licensed under CC BY-SA 3.0}

DNA is the genetic code that is carried on from cell to cell, and generation to generation. All the cells in an organism have the same genetic material. In computational biology we usually think of DNAs as long strings with \emph{T, C, G, A} as characters. But it is important to remember that for each given string, there is a complement attached to it.

\subsection{Ribonucleic acid (RNA)}
RNA molecules are polymers like DNA, but they carry uracil (U) instead of thymine (T). They are much shorted polymers compared to usual DNAs. There are different types of RNAs with different functions and messenger RNAs (mRNA) are the ones we are interested in, in the context of the central dogma. We can think of RNAs as strings of \emph{U, C, G, A}~\cite[Ch. 6]{the-cell}.

mRNAs are constructed in a process called \emph{transcription} by reading a part of a strand of the DNA and constructing its complement nucleotide by nucleotide, except whenever whenever a thymine (T) is required, instead a uracil (U) is used. This process is shown by a dark blue arrow from DNA to RNA in Figure~\ref{fig:bkg:central-dogma}~\cite[Ch. 6]{the-cell}. The term \emph{gene expression} refers to the rate at which genes are transcribed and mRNAs are synthesized from them. In other terms, a more active gene has a higher gene expression level.

\subsection{Protein}
Proteins are polymers made of amino acids. There are 20 different amino acids in humans. Proteins form and perform most of structures and functions in cells. Enzymes and cell membrane, a.k.a. cytoskeleton~\cite[Ch. 16]{the-cell}, are two examples of molecules and structures mostly made of proteins.

A process called translation, translates mRNA strands into protein strands. In this process, a ribosome complex gets attached to the start codon near the beginning of the mRNA (usually AUG), and then the mRNA strand is decoded codon by codon by tRNAs. In the decoding process, the mRNA molecule is processed three nucleotides at a time, each 3 encoding and representing a specific amino acid. A tRNA is a small RNA molecule which can attach to one codon on one side, and has an amino acid attached to its the other side. This process is continued until a stop codon (UAA, UAG, or UGA) is reached. Figure~\ref{fig:bkg:translation} illustrates a simplified version of this process. Translation is shown as a dark blue arrow from RNA to protein in Figure~\ref{fig:bkg:central-dogma}.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.6\textwidth]{figs/background/640px-Ribosome_mRNA_translation_en}
  \caption{A ribosome translating an mRNA with the help of tRNAs.}
  \label{fig:bkg:translation}
\end{figure}

\subsection{Pathways}
Cellular processes involve collaboration of several molecules (RNAs and proteins included) in the form of a long chain of reactions. A chain of reactions with a specific goal is called a pathway. Among other things, pathways can result in production of a molecule, a change in the cell, activating or deactivating a gene, or to make the cell move. A graphical representation of a pathway has molecules as nodes and reactions and dependencies as edges. Figure~\ref{fig:bkg:apoptosis-pathway} shows a graphical representation of the apoptosis pathway which results in programmed cell death, whose significance is explained shortly~\cite{bose2007p53, mott2007mir, lakin1999regulation, xiong2010effects, park2009mir}. As you can see, the same proteins and RNAs are used in different reactions, and reactions are interdependent, \emph{i.e.} the product of a reaction is a prerequisite of another reaction. It is also important to note that pathways are not mutually exclusive. The same proteins and other molecules may be used in several pathways.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=1\textwidth]{figs/background/apoptosis-pathway}
  \caption{Apoptosis (programmed cell death) pathway in homo sapiens.\protect\footnotemark}
  \label{fig:bkg:apoptosis-pathway}
\end{figure}
\footnotetext{\url{http://www.wikipathways.org/index.php/Pathway:WP254}}

\subsection{Cell Reproduction}
A new cell is created only when another cell duplicates, which itself is the result of a delicately ordered set of events and stages. The set of events ending with a cell division is called the cell cycle~\cite[Ch. 17]{the-cell}. Figure~\ref{fig:bkg:cell-cycle} illustrates the cell cycle, giving each phase approximately the time it takes the phase to complete in proportion to the whole cycle. 

\begin{figure}[!ht]
  \centering
  \includegraphics[width=.5\textwidth]{figs/background/470px-Cell_Cycle_2-2}
  \caption{Cell cycle: \emph{I}: interphase, \emph{M}: mitosis, \emph{$G_0$}: resting, \emph{$G_1$}: gap 1, \emph{S}: DNA Synthesis, \emph{$G_2$}: gap 2.\protect\footnotemark}
  \label{fig:bkg:cell-cycle}
\end{figure}
\footnotetext{Image by Zephyris (\url{https://en.wikipedia.org/wiki/DNA}) licensed under CC BY-SA 3.0}

During the \emph{S} phase (\emph{S} for DNA Synthesis) a complete copy of the cell's DNA is produced. During the \emph{M} phase, first the nucleus is devided into two, \emph{i.e.} mitosis, then the whole cell divides into two cells, \emph{i.e.} cytokinesis). For a cell which has an approximately a 24 hour cycle, the \emph{M} phase takes only one hour. $G_1$ and $G_2$ are gaps between the \emph{S} and the \emph{M} phase. During the $G_2$ phase, some processes make sure that the DNA is replicated properly and completely, and the cell is prepared to enter the \emph{M} phase. During $G_1$ phase, the cell grows, and it only enters the \emph{S} phase if the environment and conditions are favourable. $G_1$ may take a long time, and it can also enter the $G_0$ state, or resting state. A cell may stay in $G_0$ for years or even indefinitely until cell death. Human nerve cells, for instance, enter this state early in the body's development and they never duplicate. The switch between different phases are controlled by the cell's cell cycle control system which takes into account the cell's environment using the signals received from the surroundings, and the internal cell conditions using some feedbacks received from the cell's development in different stages.

\subsection{Cell Death}
Programmed cell death is as important as cell division and reproduction to a healthy tissue. An organism or a tissue can only maintain its size if cells die with the same rate as they divide, otherwise the tissue will keep growing uncontrollably. It is also critical for the cells to die in an orchestrated way during fetus development for limbs and tissues to take their desired form. A third case for programmed cell death is when cells are damaged or infected, to make sure they are removed before threatening the organism's health~\cite[Ch. 18]{the-cell}.

In most cases this programmed cell death occurs from within the cell via apoptosis, \emph{i.e.} through a set of processes and pathways which result in the cell to shrink and die and then to be eaten by other responsible cells. If the cell is large, it will be dismantled into membrane enclosed pieces. The membrane is also altered to give other cells the signal to eat them quickly. Apoptosis ensures that the contents of the cell are not spilled over other cells and that the remains are digested quickly.

In contrast to apoptosis, cells which die as a result of a physical trauma or lack of blood supply usually go through cell necrosis resulting in the swell and burst of the cell triggering an inflammatory response. As we will discuss in Section~\ref{sec:bkg:cancer}, damages to pathways related to apoptosis play a role in cancer.

\subsection{Epigenetics}
\label{sec:bkg:epigenetics}
As stated in \emph{An operational definition of epigenetics}~\cite{berger2009operational}: ``An epigenetic trait is a stably heritable phenotype resulting from changes in a chromosome without alterations in the DNA sequence''. Two of the most studied such alterations are histone modification and methylation/demethylation of the cytosine in a CG sequence, which is usually referred to as DNA-methylation. Figure~\ref{fig:bkg:epigenetics} illustrates epigenetic alterations on the DNA sequence.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=1\textwidth]{figs/background/epigeneticmechanisms}
  \caption[Epigenetic alteration mechanisms on the DNA sequence.]{Epigenetic alteration mechanisms on the DNA sequence\protect\footnotemark.}
  \label{fig:bkg:epigenetics}
\end{figure}
\footnotetext{Image available at \url{https://commonfund.nih.gov/epigenomics/figure}}

If a methyl component is attached to a DNA nucleotide, that nucleotide is called \emph{methylated}. In vertebrate cells, except stem cells, methylation happens mostly on a cytosine in a CG sequence, on both sides of the DNA strand. During cell division this property is usually kept and inherited by the offspring cells. A methylated upstream of a gene can suppress that gene's expression. In human and mice cells, it is shown that DNA methylation can inhibit the activity of an entire chromosome~\cite[Ch. 7]{the-cell}.

Histones are proteins acting as cylinders around which a DNA molecule winds and therefore is packed. As a result of this condensation, a DNA fiber of the length $180mm$ and $30nm$ is compressed into a $12\mu m$ long and $700nm$ thick molecule~\cite{redon2002histone}. Histones have a tail which can be altered, \emph{e.g.} methylation or acetylation of some of the amino acids, and some of these alterations are passed to the progenies of the cell, and hence are epigenetic markers~\cite{turner2000histone, berger2009operational, moazed2011mechanisms}. Presence, absence, or alterations of histones near a gene can activate, deactivate, or regulate the expression of that gene~\cite{turner2000histone}.


\subsection{Innate and Adaptive Immune System}
\label{sec:bkg:immune-system}
Multicellular organisms use a mechanism called innate immune response to defend themselves against pathogens, through some physical barriers such as the sweat on the skin or saliva in the mouth, as well as biochemically responding to common patterns presented by pathogens called \emph{pathogen-associated molecular patterns (PAMPs)} using receptors called \emph{pattern recognition receptors (PRRs)}~\cite[Ch. 24]{the-cell}. These PRRs detect different categories of PAMPs, triggering an inflammatory response, which in turn in animals, proceeds with the engulfment of the pathogen by a phagocytic cell. Phagocytes, macrophages, natural killer cells (NK cells), and dendritic cells are among cell types comprising the innate immune system.

Dendritic cells, present in vertebrates, are a crucial component connecting the innate immune system to the adaptive immune system. They detect a large variety of PAMPs by expressing an exhaustive variety of PRRs on their surface. Once a dendritic cell detects a pathogen or one of its products, it engulfs it through phagocytosis, through which it also becomes an activated dendritic cell. These activated cells then travel to a nearby lymphoid organ such as a lymph node to activate T-cells (discussed bellow) and present the invading pathogen to them.

At some point during the evolution, vertebrates developed an adaptive immune system, in theory capable of detecting and remembering any foreign pathogen. The core of this system are the T and B lymphocytes, commonly referred to as T and B cells. They are called T and B lymphocytes because they mature in the \emph{B}one marrow and the \emph{T}hymus respectively. Through their development, these cells undergo an inheritable somatic hyper-mutation process resulting in the expression of virtually all possible receptors and antibodies. This process, however, sometimes produces some lymphocytes detecting the organism's own cells. Therefore a separate process called immunological self-tolerance ensures lymphocytes detecting one's own cells are either destroyed or deactivated. A failure to do so causes an allergy or an autoimmune disease~\cite[Ch. 24]{the-cell}.

Both B and T cells differentiate from hematopoietic stem cells residing in bone marrow. A product of this differentiation are lymphoid progenitor cells, some of which stay in the bone marrow and develop to become B cells, and some others move to the thymus and become thymocytes and then T cells~\cite[Ch. 24]{the-cell}. The differentiation does not end at this stage, and these two types of cells will further develop into more specific cells such as effector B/T cells, na\"ive B/T cells, and memory B/T cells.

In simple terms, the adaptive immune response can be seen as two main processes. A foreign pathogen may trigger either of these two or both processes: antibody response and T cell mediated immune response. The antibody response involves B cells producing specific antibodies which bind to the targeted pathogen, disabling them from binding to the organism's own cells and also marking them for destruction. These antibodies circulate in blood and other body fluids reaching and detecting pathogens in the whole organism. T cell mediated immune response, on the other hand, works by T cells detecting some cell surface proteins called MHC proteins. These proteins are encoded in \emph{major histocompatibility complex} and are expressed in most cells of vertebrates. These MHC proteins can carry some fragments of the products of a pathogen from inside the cell to its surface. When dendritic cells present these pieces to T cells in a lymph gland, the T cells detecting those proteins are activated. These activated cells divide and differentiate into memory T cells and effector T cells. The effector T cells travel to the site of the infection and locally detect the infected cells, marking them for destruction~\cite[Ch. 24]{the-cell}.

\subsection{Cancer}
\label{sec:bkg:cancer}
Thinking of an organism as a society of cells helps to better understand and explain cancer. In a healthy and functional organism, cells collaborate, do not invade each other's space, and sacrifice themselves through programmed cell death to control their own population. A group of cells reproducing abnormally and faster than the usual rate leads to a neoplasm. If these cells are not invasive to the neighboring cells ,the formed tumor is called benign. On the other hand, if the cells start reproducing as wells as invading the surrounding tissue, it then is called a malignant tumor, or cancer~\cite[Ch. 20]{the-cell}.

A malfunction in a pathway related to the cell cycle can cause cells to reproduce uncontrollably and/or not die according to the plan. These malfunctions can be caused by over or under expression of a gene resulting in excess or lack of a protein or an RNA in a pathway. A mutation in the upstream of a gene, e.g. the promoter region of the gene, or certain epigenetic changes such as methylation of the upstream region of a gene can lead to under expression or completely disable the expression of that gene. On the other hand, demethylation of the promoter region might enhance the expression of the gene and disrupt the related pathways. It can also be the case that a mutation on a specific gene renders a protein dysfunctional, hence a malfunction in the pathways to which it belongs~\cite{esteller2008epigenetics}.

Even without the presence of mutation inducing agents, mutations naturally occur on the DNA during cell division, most of which are corrected due to DNA repair mechanisms present in the cell. Out of the mutations which remain on the DNA, only a few can lead to a malignant cell. There is also strong evidence that some cancer causing mutations need to be present on both chromosomes for it to cause cancer, unless that mutation already exists on one of the chromosomes, inherited from one of the parents; in which case, only one mutation on the right position is enough to activate/deactivate that corresponding gene~\cite{the-emperor-of-all-maladies}.

Cancer cells also go though some morphological changes which are visible under a microscope. These cells have a larger nucleus and a smaller cytoplasm, therefore a larger nucleus/cytoplasm ratio, and both nucleus and the cell have irregular shapes. Investigation of these characteristics play a role in cancer diagnosis~\cite{baba2007tumor}. Figure~\ref{fig:bkg:normal-cancer} depicts some of the differences between normal and cancer cells.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=.8\textwidth]{figs/background/Normal_and_cancer_cells_structure}
  \caption{Some morphological differences between normal and cancer cells.}
  \label{fig:bkg:normal-cancer}
\end{figure}

Since cancer cells are evolved from the cells of the organism itself, they are not detected by the immune system as enemies. These cancerous cells also know all the internal protocols of the organism. For instance, as soon as a cancerous cell grows into a tumor-like mass, it needs more blood supply. Therefore it sends signals triggering new veins to develop to support the tumor with the food and oxygen it needs. The cancerous cells may then enter the blood stream of lymphatic vessels, land on another part of the body, and form a secondary tumor, \emph{i.e.} metastasis~\cite[Ch. 20]{the-cell}.

\subsection{Lymphoid Neoplasms and Lymphoma}
Like other diseases, cancer had traditionally been categorized according to the symptoms and the organ in which it appears. With the advancements in medicine, we could detect metastasis and therefore classify the disease according to the organ from which it originates. This classification, however, is far from perfect. Depending on the underlying genetic cause of the cancer, it can practically be considered different diseases although happening on the same organ. For instance, breast cancer was considered one disease for thousands of years, until it was discovered that tumor cells with estrogen receptors on their surface as inhibitors, \emph{i.e.} ER+ breast cancer tumors, are a different disease than the ones not having estrogen as an inhibitor. This discovery resulted in subclassification of cancer into ER+ and ER- tumors improving diagnosis, treatment, and prognosis specially for patients with ER+ tumors~\cite{the-emperor-of-all-maladies}.

When it comes to the cancers of the immune system, a few factors make the classification of the disease even more complicated. One is the location of the cancer, which can be any place along the development path of our immune system, bone marrow, thymus, blood stream, and lymph nodes included. For this reason, cancers of the immune system used to be put in two main classes: (1) if the disease appears only in blood stream and does not form a mass: leukemia (2) if it forms a mass mainly where lymph nodes are located: lymphoma~\cite{swerdlow20162016, younes2016handbook}. However, our better understanding of the immune system and these diseases proved traditional classification of these diseases inefficient at best, and as a result, WHO classification of these malignancies now puts them all together under the category of \emph{lymphoid neoplasms}~\cite{norris2008classification, swerdlow20162016}. For example, Chronic lymphocytic leukemia and small lymphocytic lymphoma are now considered two different stages of the same disease affecting mature B cells. Generally speaking, lymphoma are neoplasms of the immune system resulting in a solid tumor~\cite[Ch. 1]{younes2016handbook}.

As mentioned in Section~\ref{sec:bkg:immune-system}, B and T cells both undergo somatic hypermutation. B cells, for example, once activated by an antigen, undergo a somatic mutation phase with a rate at least 100,000 times higher than the normal rate of mutation across the genome~\cite{mendelsohn2014molecular}. Although the hypermutation phase is highly regulated, sometimes errors happen which can lead to a possible malignancy.

The first level of lymphoma classification creates two main categories: Hodgkin and non-Hodgkin lymphoma (NHL). The disease is considered Hodgkin lymphoma if Reed-Sternberg cells are present in the biopsy. These cells are large lymphocytes usually derived from B cells and can have more than one nucleus, as shown in Figure~\ref{fig:bkg:rs-cell}~\cite{sternberg1898eigenartige}.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=.8\textwidth]{figs/background/Reed-Sternberg_lymphocyte_nci-vol-7172-300}
  \caption{Reed-Sternberg cells in a sea of normal lymphocytes, indicative of Hodgkin lymphoma.}
  \label{fig:bkg:rs-cell}
\end{figure}

Like Hodgkin lymphoma, the classification of non-Hodgkin cases also started by focusing on morphological features of the cancer cells. However, through improved understanding of the disease, immunologic, genetic, and clinical criteria are now also considered to better distinguish between cancer subtypes~\cite[Ch. 33]{hoffbrand2016postgraduate}. For instance, in the case of anaplastic large T cell lymphoma (ALCL), there are treatments if an anaplastic lymphoma kinase (ALK) translocation (t[2,5]) is present in the neoplasm, leading to a much better prognosis. As a result, a classification of ALCL now includes ALK$^+$ and ALK$^-$ subtypes~\cite[Ch. 29]{mendelsohn2014molecular}.
Our focus in this work is on two major types of mature B cell neoplasms, namely Follicular Lymphoma (FL) and Diffuse Large B Cell Lymphomas (DLBCL).

DLBCL is a high grade aggressive NHL and the most common among NHL cases. FL on the other hand, is the most common low grade NHL and is only second to DLBCL in terms of the number of NHL cases. Both FL and DLBCL are heterogeneous diseases and have several subtypes of their own. FL is defined as a neoplasm of B cells in germinal centers, comprising different proportions of small centrocytes and large centroblasts with a follicular growth pattern, as shown in Figure~\ref{fig:bkg:fl}~\cite[Ch. 31]{hoffbrand2016postgraduate}. Its cases are divided into three main grades, and the third grade divided into 3a and 3b. Except FL-3b, the other cases progress slowly and all have a similar prognosis outcome. Grade 3b, on the other hand, is more aggressive and is clinically closer to DLBCL~\cite[Ch. 33]{hoffbrand2016postgraduate}.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=1\textwidth]{figs/background/fl}
  \caption{Follicular lymphoma. (a) The tumour grows in a follicular pattern with expanded germinal centres without macrophages. The
tumour cells are positive for CD10 (b) and BCL-2 (c).\protect\footnotemark}
  \label{fig:bkg:fl}
\end{figure}
\footnotetext{Image and caption taken from \emph{Postgraduate Haematology}, p. 582~\cite[Ch. 31]{hoffbrand2016postgraduate}.}

DLBCL is a neoplasm of B cells with the commonality of having a diffuse growth pattern. It includes a very diverse set of subtypes. There are also classes of neoplasms categorized between DLBCL and other types of lymphoid neoplasms such as Hodgkin lymphoma or Burkitt lymphoma. Although in many cases it is not clear what the background of the occurring DLBCL is, in some rather rare cases it can be a transformation from other diseases such as FL~\cite[Ch. 31, 33]{hoffbrand2016postgraduate}.

Since many lymphoid neoplasms are heterogeneous diseases each with a different underlying genetic cause, accurate classification of each case using gene expression profiling (GEP) or next generation sequencing (NGS) enables oncologists and pathologists to target specific genes, inhibiting or activating them, in order to activate apoptosis or inhibit cell growth as a method of treatment. However, since it is not feasible to use GEP or NGS for all patients due to their complexity and costs, immunophenotyping is often used as a proxy to diagnose each patient's subtype. In this work, Chapter~\ref{sec:fcs} deals with immunophenotype data, and then Chapter~\ref{sec:adaptive-learning} explores analysis of genetic and epigenetics data retrieved from biopsies of cancer patients.
